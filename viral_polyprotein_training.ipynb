{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bf61a8f",
   "metadata": {},
   "source": [
    "# Viral Polyprotein Cleavage Site Prediction - Data Download & Training\n",
    "\n",
    "This notebook demonstrates the complete workflow for:\n",
    "1. Downloading viral polyprotein data from RefSeq\n",
    "2. Processing and validating the data\n",
    "3. Training a machine learning model for cleavage site prediction\n",
    "\n",
    "## Overview\n",
    "\n",
    "Viral polyproteins are large precursor proteins that are cleaved into functional mature proteins. Accurate prediction of cleavage sites is crucial for understanding viral protein processing and can aid in antiviral drug development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cb5d03",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, let's import all the necessary libraries for data download, processing, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b245049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Our custom data preparation module\n",
    "from data_prep import (\n",
    "    download_specific_viral_families,\n",
    "    create_train_val_test_splits,\n",
    "    validate_data_format\n",
    ")\n",
    "\n",
    "print(\"‚úì All libraries imported successfully!\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69e352b",
   "metadata": {},
   "source": [
    "## 2. Download and Load Data\n",
    "\n",
    "Let's download viral polyprotein data from RefSeq using our enhanced search terms. We'll target multiple viral families to get a diverse dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0c9545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for data download\n",
    "VIRAL_FAMILIES = [\n",
    "    'Coronaviridae',    # SARS-CoV-2, MERS, etc.\n",
    "    'Picornaviridae',   # Poliovirus, rhinovirus\n",
    "    'Flaviviridae',     # Dengue, Zika, HCV\n",
    "    'Caliciviridae',    # Norovirus\n",
    "    'Arteriviridae',    # PRRSV\n",
    "]\n",
    "\n",
    "OUTPUT_FILE = \"viral_polyproteins_dataset.json\"\n",
    "MAX_PER_FAMILY = 10  # Limit per family for this demo\n",
    "EMAIL = \"demo@example.com\"  # Replace with your email\n",
    "\n",
    "print(\"ü¶† Starting data download from RefSeq...\")\n",
    "print(f\"Target viral families: {VIRAL_FAMILIES}\")\n",
    "print(f\"Max entries per family: {MAX_PER_FAMILY}\")\n",
    "\n",
    "# Download the data\n",
    "try:\n",
    "    download_specific_viral_families(\n",
    "        viral_families=VIRAL_FAMILIES,\n",
    "        output_file=OUTPUT_FILE,\n",
    "        max_per_family=MAX_PER_FAMILY,\n",
    "        email=EMAIL\n",
    "    )\n",
    "    print(f\"‚úì Data downloaded successfully to {OUTPUT_FILE}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error downloading data: {e}\")\n",
    "    print(\"Note: This requires internet connection to NCBI RefSeq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f1eff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and examine the downloaded data\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    with open(OUTPUT_FILE, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"üìä Dataset Summary:\")\n",
    "    print(f\"Total polyproteins: {len(data)}\")\n",
    "    \n",
    "    if len(data) > 0:\n",
    "        # Analyze by viral family\n",
    "        families = [entry['viral_family'] for entry in data]\n",
    "        family_counts = Counter(families)\n",
    "        \n",
    "        print(f\"\\nBy viral family:\")\n",
    "        for family, count in family_counts.items():\n",
    "            print(f\"  ‚Ä¢ {family}: {count} polyproteins\")\n",
    "        \n",
    "        # Analyze cleavage sites\n",
    "        cleavage_counts = [len(entry['cleavage_sites']) for entry in data]\n",
    "        total_sites = sum(cleavage_counts)\n",
    "        \n",
    "        print(f\"\\nCleavage site statistics:\")\n",
    "        print(f\"  ‚Ä¢ Total cleavage sites: {total_sites}\")\n",
    "        print(f\"  ‚Ä¢ Average sites per protein: {np.mean(cleavage_counts):.1f}\")\n",
    "        print(f\"  ‚Ä¢ Range: {min(cleavage_counts)} - {max(cleavage_counts)} sites\")\n",
    "        \n",
    "        # Sequence length analysis\n",
    "        seq_lengths = [len(entry['sequence']) for entry in data]\n",
    "        print(f\"\\nSequence length statistics:\")\n",
    "        print(f\"  ‚Ä¢ Average length: {np.mean(seq_lengths):.0f} amino acids\")\n",
    "        print(f\"  ‚Ä¢ Range: {min(seq_lengths)} - {max(seq_lengths)} amino acids\")\n",
    "        \n",
    "        # Show a sample entry\n",
    "        sample = data[0]\n",
    "        print(f\"\\nüìã Sample entry:\")\n",
    "        print(f\"  ‚Ä¢ ID: {sample['protein_id']}\")\n",
    "        print(f\"  ‚Ä¢ Organism: {sample['organism']}\")\n",
    "        print(f\"  ‚Ä¢ Family: {sample['viral_family']}\")\n",
    "        print(f\"  ‚Ä¢ Sequence length: {len(sample['sequence'])} aa\")\n",
    "        print(f\"  ‚Ä¢ Cleavage sites: {sample['cleavage_sites']}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No data file found. Please run the download cell first.\")\n",
    "    data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7820cb41",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "Now let's process the raw polyprotein data to create features suitable for machine learning. We'll create a dataset where each amino acid position is labeled as either a cleavage site (1) or non-cleavage site (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ba650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequence_features(sequence, window_size=5):\n",
    "    \"\"\"\n",
    "    Create features for each position in a protein sequence using a sliding window.\n",
    "    \n",
    "    Args:\n",
    "        sequence: Protein sequence string\n",
    "        window_size: Size of the window around each position\n",
    "    \n",
    "    Returns:\n",
    "        List of feature dictionaries, one per position\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    half_window = window_size // 2\n",
    "    \n",
    "    # Pad sequence with special characters\n",
    "    padded_seq = 'X' * half_window + sequence + 'X' * half_window\n",
    "    \n",
    "    for i in range(half_window, len(padded_seq) - half_window):\n",
    "        # Extract window around position\n",
    "        window = padded_seq[i - half_window:i + half_window + 1]\n",
    "        \n",
    "        # Create features\n",
    "        feature_dict = {\n",
    "            'center_aa': padded_seq[i],\n",
    "            'window': window,\n",
    "            'position': i - half_window,  # 0-indexed position in original sequence\n",
    "            'rel_position': (i - half_window) / len(sequence),  # Relative position\n",
    "        }\n",
    "        \n",
    "        # Add amino acid composition features\n",
    "        for aa in 'ACDEFGHIKLMNPQRSTVWY':\n",
    "            feature_dict[f'aa_{aa}'] = window.count(aa) / len(window)\n",
    "        \n",
    "        features.append(feature_dict)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def process_polyprotein_data(data):\n",
    "    \"\"\"\n",
    "    Convert polyprotein data into a machine learning dataset.\n",
    "    \n",
    "    Args:\n",
    "        data: List of polyprotein dictionaries\n",
    "    \n",
    "    Returns:\n",
    "        pandas DataFrame with features and labels\n",
    "    \"\"\"\n",
    "    all_features = []\n",
    "    \n",
    "    for entry in data:\n",
    "        sequence = entry['sequence']\n",
    "        cleavage_sites = set(entry['cleavage_sites'])\n",
    "        viral_family = entry['viral_family']\n",
    "        organism = entry['organism']\n",
    "        \n",
    "        # Get features for each position\n",
    "        seq_features = create_sequence_features(sequence)\n",
    "        \n",
    "        # Add labels and metadata\n",
    "        for feature in seq_features:\n",
    "            position = feature['position']\n",
    "            feature['is_cleavage'] = 1 if position in cleavage_sites else 0\n",
    "            feature['viral_family'] = viral_family\n",
    "            feature['organism'] = organism\n",
    "            feature['protein_id'] = entry['protein_id']\n",
    "            feature['sequence_length'] = len(sequence)\n",
    "            \n",
    "            all_features.append(feature)\n",
    "    \n",
    "    return pd.DataFrame(all_features)\n",
    "\n",
    "# Process the data\n",
    "if len(data) > 0:\n",
    "    print(\"üîÑ Processing polyprotein data for machine learning...\")\n",
    "    df = process_polyprotein_data(data)\n",
    "    \n",
    "    print(f\"‚úì Created dataset with {len(df)} amino acid positions\")\n",
    "    print(f\"‚úì Features per position: {len(df.columns)}\")\n",
    "    print(f\"‚úì Cleavage sites: {df['is_cleavage'].sum()}\")\n",
    "    print(f\"‚úì Non-cleavage sites: {(df['is_cleavage'] == 0).sum()}\")\n",
    "    print(f\"‚úì Class balance: {df['is_cleavage'].mean():.3f} cleavage rate\")\n",
    "    \n",
    "    # Display the first few rows\n",
    "    print(f\"\\nüìä Sample of processed data:\")\n",
    "    display_cols = ['protein_id', 'position', 'center_aa', 'window', 'is_cleavage', 'viral_family']\n",
    "    print(df[display_cols].head(10))\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data to process\")\n",
    "    df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec06366",
   "metadata": {},
   "source": [
    "## 4. Model Training & Evaluation\n",
    "\n",
    "Now let's split the data and train machine learning models to predict cleavage sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f71918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "def prepare_features(df):\n",
    "    \"\"\"\n",
    "    Prepare features for machine learning models.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame from process_polyprotein_data\n",
    "    \n",
    "    Returns:\n",
    "        X: Feature matrix\n",
    "        y: Target labels\n",
    "        feature_names: List of feature names\n",
    "    \"\"\"\n",
    "    if len(df) == 0:\n",
    "        return np.array([]), np.array([]), []\n",
    "    \n",
    "    # Select amino acid composition features\n",
    "    aa_features = [col for col in df.columns if col.startswith('aa_')]\n",
    "    \n",
    "    # Additional numerical features\n",
    "    numerical_features = ['rel_position', 'sequence_length']\n",
    "    \n",
    "    # Categorical features (encoded)\n",
    "    categorical_features = []\n",
    "    df_encoded = df.copy()\n",
    "    \n",
    "    # Encode viral family\n",
    "    if 'viral_family' in df.columns:\n",
    "        le_family = LabelEncoder()\n",
    "        df_encoded['viral_family_encoded'] = le_family.fit_transform(df['viral_family'])\n",
    "        categorical_features.append('viral_family_encoded')\n",
    "    \n",
    "    # Combine all features\n",
    "    feature_cols = aa_features + numerical_features + categorical_features\n",
    "    X = df_encoded[feature_cols].values\n",
    "    y = df['is_cleavage'].values\n",
    "    \n",
    "    return X, y, feature_cols\n",
    "\n",
    "def train_and_evaluate_models(X, y, feature_names):\n",
    "    \"\"\"\n",
    "    Train and evaluate multiple models using cross-validation.\n",
    "    \"\"\"\n",
    "    if len(X) == 0:\n",
    "        print(\"‚ùå No data available for training\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üèÉ Training models on {len(X)} samples with {X.shape[1]} features\")\n",
    "    print(f\"üìä Class distribution: {np.bincount(y)}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Define models\n",
    "    models = {\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\n",
    "        'Logistic Regression': LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nüî¨ Training {name}...\")\n",
    "        \n",
    "        # Use scaled features for Logistic Regression, original for Random Forest\n",
    "        X_train_model = X_train_scaled if 'Logistic' in name else X_train\n",
    "        X_test_model = X_test_scaled if 'Logistic' in name else X_test\n",
    "        \n",
    "        # Train model\n",
    "        model.fit(X_train_model, y_train)\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_scores = cross_val_score(\n",
    "            model, X_train_model, y_train, \n",
    "            cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "            scoring='roc_auc'\n",
    "        )\n",
    "        \n",
    "        # Test predictions\n",
    "        y_pred = model.predict(X_test_model)\n",
    "        y_pred_proba = model.predict_proba(X_test_model)[:, 1]\n",
    "        \n",
    "        # Metrics\n",
    "        test_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        \n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'cv_auc_mean': cv_scores.mean(),\n",
    "            'cv_auc_std': cv_scores.std(),\n",
    "            'test_auc': test_auc,\n",
    "            'y_test': y_test,\n",
    "            'y_pred': y_pred,\n",
    "            'y_pred_proba': y_pred_proba\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úì CV AUC: {cv_scores.mean():.3f} ¬± {cv_scores.std():.3f}\")\n",
    "        print(f\"‚úì Test AUC: {test_auc:.3f}\")\n",
    "        print(f\"‚úì Classification Report:\")\n",
    "        print(classification_report(y_test, y_pred, digits=3))\n",
    "    \n",
    "    return results, scaler, feature_names\n",
    "\n",
    "# Train models if data is available\n",
    "if len(df) > 0:\n",
    "    print(\"üöÄ Preparing features and training models...\")\n",
    "    X, y, feature_names = prepare_features(df)\n",
    "    \n",
    "    if len(X) > 0:\n",
    "        model_results, scaler, feature_names = train_and_evaluate_models(X, y, feature_names)\n",
    "        print(\"‚úÖ Model training completed!\")\n",
    "    else:\n",
    "        print(\"‚ùå No features could be prepared\")\n",
    "        model_results = {}\n",
    "else:\n",
    "    print(\"‚ùå No data available for training\")\n",
    "    model_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaf6360",
   "metadata": {},
   "source": [
    "## 5. Results Visualization\n",
    "\n",
    "Let's visualize the model performance and feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e455b9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_results(model_results):\n",
    "    \"\"\"\n",
    "    Create visualizations for model performance.\n",
    "    \"\"\"\n",
    "    if not model_results:\n",
    "        print(\"‚ùå No model results to plot\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. ROC Curves\n",
    "    ax1 = axes[0, 0]\n",
    "    for name, results in model_results.items():\n",
    "        y_test = results['y_test']\n",
    "        y_pred_proba = results['y_pred_proba']\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        auc = results['test_auc']\n",
    "        \n",
    "        ax1.plot(fpr, tpr, label=f'{name} (AUC = {auc:.3f})', linewidth=2)\n",
    "    \n",
    "    ax1.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    ax1.set_xlabel('False Positive Rate')\n",
    "    ax1.set_ylabel('True Positive Rate')\n",
    "    ax1.set_title('ROC Curves')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Cross-validation AUC comparison\n",
    "    ax2 = axes[0, 1]\n",
    "    names = list(model_results.keys())\n",
    "    cv_means = [model_results[name]['cv_auc_mean'] for name in names]\n",
    "    cv_stds = [model_results[name]['cv_auc_std'] for name in names]\n",
    "    \n",
    "    bars = ax2.bar(names, cv_means, yerr=cv_stds, capsize=5, alpha=0.7)\n",
    "    ax2.set_ylabel('Cross-validation AUC')\n",
    "    ax2.set_title('Model Comparison (5-fold CV)')\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, mean in zip(bars, cv_means):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{mean:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Confusion Matrix (for Random Forest)\n",
    "    if 'Random Forest' in model_results:\n",
    "        ax3 = axes[1, 0]\n",
    "        rf_results = model_results['Random Forest']\n",
    "        cm = confusion_matrix(rf_results['y_test'], rf_results['y_pred'])\n",
    "        \n",
    "        im = ax3.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        ax3.figure.colorbar(im, ax=ax3)\n",
    "        \n",
    "        # Add text annotations\n",
    "        thresh = cm.max() / 2.\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                ax3.text(j, i, format(cm[i, j], 'd'),\n",
    "                        ha=\"center\", va=\"center\",\n",
    "                        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        \n",
    "        ax3.set_xlabel('Predicted Label')\n",
    "        ax3.set_ylabel('True Label')\n",
    "        ax3.set_title('Confusion Matrix (Random Forest)')\n",
    "        ax3.set_xticks([0, 1])\n",
    "        ax3.set_yticks([0, 1])\n",
    "        ax3.set_xticklabels(['Not Cleavage', 'Cleavage'])\n",
    "        ax3.set_yticklabels(['Not Cleavage', 'Cleavage'])\n",
    "    \n",
    "    # 4. Feature Importance (Random Forest)\n",
    "    if 'Random Forest' in model_results and 'feature_names' in globals():\n",
    "        ax4 = axes[1, 1]\n",
    "        rf_model = model_results['Random Forest']['model']\n",
    "        importances = rf_model.feature_importances_\n",
    "        \n",
    "        # Get top 15 most important features\n",
    "        indices = np.argsort(importances)[::-1][:15]\n",
    "        top_features = [feature_names[i] for i in indices]\n",
    "        top_importances = importances[indices]\n",
    "        \n",
    "        y_pos = np.arange(len(top_features))\n",
    "        ax4.barh(y_pos, top_importances, alpha=0.7)\n",
    "        ax4.set_yticks(y_pos)\n",
    "        ax4.set_yticklabels(top_features)\n",
    "        ax4.set_xlabel('Feature Importance')\n",
    "        ax4.set_title('Top 15 Feature Importances (Random Forest)')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Reverse y-axis to show most important at top\n",
    "        ax4.invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_cleavage_site_analysis(df):\n",
    "    \"\"\"\n",
    "    Analyze cleavage site patterns in the data.\n",
    "    \"\"\"\n",
    "    if len(df) == 0:\n",
    "        print(\"‚ùå No data to analyze\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Cleavage sites by viral family\n",
    "    ax1 = axes[0, 0]\n",
    "    cleavage_by_family = df.groupby('viral_family')['is_cleavage'].agg(['sum', 'count', 'mean'])\n",
    "    cleavage_by_family['rate'] = cleavage_by_family['mean']\n",
    "    \n",
    "    bars = ax1.bar(cleavage_by_family.index, cleavage_by_family['rate'], alpha=0.7)\n",
    "    ax1.set_ylabel('Cleavage Site Rate')\n",
    "    ax1.set_title('Cleavage Site Rate by Viral Family')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, rate in zip(bars, cleavage_by_family['rate']):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "                f'{rate:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Amino acid frequency at cleavage sites\n",
    "    ax2 = axes[0, 1]\n",
    "    cleavage_aa = df[df['is_cleavage'] == 1]['center_aa'].value_counts()\n",
    "    non_cleavage_aa = df[df['is_cleavage'] == 0]['center_aa'].value_counts()\n",
    "    \n",
    "    # Normalize by total counts\n",
    "    cleavage_freq = cleavage_aa / cleavage_aa.sum()\n",
    "    non_cleavage_freq = non_cleavage_aa / non_cleavage_aa.sum()\n",
    "    \n",
    "    all_aa = sorted(set(cleavage_freq.index) | set(non_cleavage_freq.index))\n",
    "    cleavage_vals = [cleavage_freq.get(aa, 0) for aa in all_aa]\n",
    "    non_cleavage_vals = [non_cleavage_freq.get(aa, 0) for aa in all_aa]\n",
    "    \n",
    "    x = np.arange(len(all_aa))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax2.bar(x - width/2, cleavage_vals, width, label='Cleavage Sites', alpha=0.7)\n",
    "    ax2.bar(x + width/2, non_cleavage_vals, width, label='Non-cleavage Sites', alpha=0.7)\n",
    "    \n",
    "    ax2.set_xlabel('Amino Acid')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_title('Amino Acid Frequency at Cleavage vs Non-cleavage Sites')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(all_aa)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Cleavage site position distribution\n",
    "    ax3 = axes[1, 0]\n",
    "    cleavage_positions = df[df['is_cleavage'] == 1]['rel_position']\n",
    "    \n",
    "    ax3.hist(cleavage_positions, bins=20, alpha=0.7, edgecolor='black')\n",
    "    ax3.set_xlabel('Relative Position in Sequence')\n",
    "    ax3.set_ylabel('Number of Cleavage Sites')\n",
    "    ax3.set_title('Distribution of Cleavage Sites by Position')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Sequence length distribution\n",
    "    ax4 = axes[1, 1]\n",
    "    unique_sequences = df.drop_duplicates('protein_id')\n",
    "    \n",
    "    ax4.hist(unique_sequences['sequence_length'], bins=15, alpha=0.7, edgecolor='black')\n",
    "    ax4.set_xlabel('Sequence Length')\n",
    "    ax4.set_ylabel('Number of Sequences')\n",
    "    ax4.set_title('Distribution of Sequence Lengths')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate plots if data and results are available\n",
    "if len(df) > 0:\n",
    "    print(\"üìä Creating visualizations...\")\n",
    "    \n",
    "    # Plot model results\n",
    "    if model_results:\n",
    "        plot_model_results(model_results)\n",
    "    \n",
    "    # Plot data analysis\n",
    "    plot_cleavage_site_analysis(df)\n",
    "    \n",
    "    print(\"‚úÖ Visualization completed!\")\n",
    "else:\n",
    "    print(\"‚ùå No data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb61649",
   "metadata": {},
   "source": [
    "## 6. Summary & Next Steps\n",
    "\n",
    "Let's summarize our findings and suggest improvements for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e9fe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and analysis\n",
    "if len(df) > 0 and model_results:\n",
    "    print(\"üéØ VIRAL POLYPROTEIN CLEAVAGE PREDICTION - SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Dataset summary\n",
    "    print(f\"\\nüìä DATASET SUMMARY:\")\n",
    "    print(f\"   ‚Ä¢ Total amino acid positions: {len(df):,}\")\n",
    "    print(f\"   ‚Ä¢ Unique proteins: {df['protein_id'].nunique()}\")\n",
    "    print(f\"   ‚Ä¢ Viral families: {df['viral_family'].nunique()}\")\n",
    "    print(f\"   ‚Ä¢ Cleavage sites: {df['is_cleavage'].sum():,}\")\n",
    "    print(f\"   ‚Ä¢ Cleavage rate: {df['is_cleavage'].mean():.1%}\")\n",
    "    \n",
    "    # Viral family breakdown\n",
    "    print(f\"\\nü¶† VIRAL FAMILIES:\")\n",
    "    family_stats = df.groupby('viral_family').agg({\n",
    "        'protein_id': 'nunique',\n",
    "        'is_cleavage': ['sum', 'mean']\n",
    "    }).round(3)\n",
    "    \n",
    "    for family in family_stats.index:\n",
    "        proteins = family_stats.loc[family, ('protein_id', 'nunique')]\n",
    "        cleavage_sites = family_stats.loc[family, ('is_cleavage', 'sum')]\n",
    "        cleavage_rate = family_stats.loc[family, ('is_cleavage', 'mean')]\n",
    "        print(f\"   ‚Ä¢ {family}: {proteins} proteins, {cleavage_sites} sites ({cleavage_rate:.1%})\")\n",
    "    \n",
    "    # Model performance\n",
    "    print(f\"\\nü§ñ MODEL PERFORMANCE:\")\n",
    "    for name, results in model_results.items():\n",
    "        cv_auc = results['cv_auc_mean']\n",
    "        test_auc = results['test_auc']\n",
    "        print(f\"   ‚Ä¢ {name}:\")\n",
    "        print(f\"     - Cross-validation AUC: {cv_auc:.3f} ¬± {results['cv_auc_std']:.3f}\")\n",
    "        print(f\"     - Test AUC: {test_auc:.3f}\")\n",
    "    \n",
    "    # Feature insights (Random Forest)\n",
    "    if 'Random Forest' in model_results and 'feature_names' in globals():\n",
    "        rf_model = model_results['Random Forest']['model']\n",
    "        importances = rf_model.feature_importances_\n",
    "        \n",
    "        print(f\"\\nüîç TOP PREDICTIVE FEATURES:\")\n",
    "        indices = np.argsort(importances)[::-1][:5]\n",
    "        for i, idx in enumerate(indices):\n",
    "            feature = feature_names[idx]\n",
    "            importance = importances[idx]\n",
    "            print(f\"   {i+1}. {feature}: {importance:.3f}\")\n",
    "    \n",
    "    print(f\"\\nüí° INSIGHTS:\")\n",
    "    \n",
    "    # Class imbalance insight\n",
    "    cleavage_rate = df['is_cleavage'].mean()\n",
    "    if cleavage_rate < 0.1:\n",
    "        print(f\"   ‚Ä¢ High class imbalance ({cleavage_rate:.1%} cleavage sites)\")\n",
    "        print(f\"   ‚Ä¢ Used balanced class weights to address this\")\n",
    "    \n",
    "    # Performance insight\n",
    "    best_model = max(model_results.items(), key=lambda x: x[1]['test_auc'])\n",
    "    best_name, best_results = best_model\n",
    "    if best_results['test_auc'] > 0.8:\n",
    "        print(f\"   ‚Ä¢ {best_name} shows good performance (AUC = {best_results['test_auc']:.3f})\")\n",
    "    elif best_results['test_auc'] > 0.7:\n",
    "        print(f\"   ‚Ä¢ {best_name} shows moderate performance (AUC = {best_results['test_auc']:.3f})\")\n",
    "    else:\n",
    "        print(f\"   ‚Ä¢ Models show limited performance (best AUC = {best_results['test_auc']:.3f})\")\n",
    "    \n",
    "    print(f\"\\nüöÄ NEXT STEPS:\")\n",
    "    print(f\"   1. Collect more data (especially from underrepresented families)\")\n",
    "    print(f\"   2. Try ensemble methods or deep learning approaches\")\n",
    "    print(f\"   3. Include structural features (secondary structure, surface accessibility)\")\n",
    "    print(f\"   4. Experiment with different window sizes and feature engineering\")\n",
    "    print(f\"   5. Use domain-specific features (protease specificity motifs)\")\n",
    "    print(f\"   6. Cross-validate across viral families to test generalization\")\n",
    "    \n",
    "    print(f\"\\nüìö RESOURCES:\")\n",
    "    print(f\"   ‚Ä¢ MEROPS database: https://www.ebi.ac.uk/merops/\")\n",
    "    print(f\"   ‚Ä¢ UniProt viral proteomes: https://www.uniprot.org/proteomes/\")\n",
    "    print(f\"   ‚Ä¢ PDB structures for 3D features: https://www.rcsb.org/\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data or model results available for summary\")\n",
    "\n",
    "print(f\"\\n‚úÖ Notebook execution completed!\")\n",
    "print(f\"üìù Check the plots above for detailed visualizations\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
